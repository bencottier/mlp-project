Notes for parallel loading of the datasets

#How to preprocess?
1. Segregate the data as indicated in config-preprocess.yml
2. Pre-process as before. It will build several files in the processed_data directory

**********************************************************************************************************
#How to train with multiple datasets?
1. Use the following arguments in config file for train:
   data_ids: [E, M, H] 
   data_weights: [1,1,1] #It needs to be fixed as it alternates the iterators 
   accum_count: 3 

2. accum_count will load 3 batches in 1 step whose loss will be calculated together. In our case, it will load easy, medium and hard batch in "batches" variable at the following line in trainer.py: 
for i, (batches, normalization) in enumerate(
                self._accum_batches(train_iter)): 

**********************************************************************************************************
#How to check batch content?
-Use following piece of code where batch is the Batch object in pytorch.

sen_no = 1 # Index in the batch to print
inds, perm = torch.sort(batch.indices)
src = batch.src[0][:, :, 0].index_select(1, perm)
src_raw = batch.dataset.examples[inds[sen_no]].src[0]
print('Sentence: ',src_raw) 
**********************************************************************************************************  
#Changes made for above in the code:
-I just changed single file at /onmt/inputters/inputter.py 
